# Compass - Audytor SEO/AEO/GEO

Zaawansowany audytor SEO/AEO/GEO z moduÅ‚owÄ… architekturÄ…, wsparciem dla E-E-A-T, Local SEO (NAP), bezpieczeÅ„stwa i AI-powered podsumowaÅ„.

## ğŸš€ Funkcje

- **SEO**: Analiza meta tagÃ³w, nagÅ‚Ã³wkÃ³w, canonical, duplicates
- **AEO/GEO**: Schema.org, strukturalne dane, sygnaÅ‚y E-E-A-T
- **Local SEO**: Analiza NAP (Name, Address, Phone)
- **BezpieczeÅ„stwo**: Audyt nagÅ‚Ã³wkÃ³w HTTP, SSL, mixed content
- **AI Summary**: Automatyczne podsumowania przy uÅ¼yciu OpenAI
- **PageSpeed**: Integracja z Google PageSpeed Insights
- **Raporty**: JSON, CSV, Word (DOCX)

## ğŸ“ Struktura projektu

```
Compass/
â”œâ”€â”€ compass/                    # GÅ‚Ã³wny pakiet
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Konfiguracja
â”‚   â”œâ”€â”€ utils/                 # NarzÄ™dzia pomocnicze
â”‚   â”‚   â”œâ”€â”€ url_utils.py       # Operacje na URL
â”‚   â”‚   â””â”€â”€ text_utils.py      # Przetwarzanie tekstu
â”‚   â”œâ”€â”€ analyzers/             # Analizatory SEO/AEO/Security
â”‚   â”‚   â”œâ”€â”€ meta_analyzer.py
â”‚   â”‚   â”œâ”€â”€ nap_analyzer.py
â”‚   â”‚   â”œâ”€â”€ eeat_analyzer.py
â”‚   â”‚   â””â”€â”€ security_analyzer.py
â”‚   â”œâ”€â”€ crawler/               # ModuÅ‚ crawlera
â”‚   â”‚   â”œâ”€â”€ fetcher.py         # Pobieranie stron
â”‚   â”‚   â”œâ”€â”€ robots.py          # robots.txt i sitemap
â”‚   â”‚   â””â”€â”€ crawler.py         # GÅ‚Ã³wny crawler
â”‚   â”œâ”€â”€ integrations/          # Integracje zewnÄ™trzne
â”‚   â”‚   â”œâ”€â”€ openai_integration.py
â”‚   â”‚   â””â”€â”€ pagespeed.py
â”‚   â””â”€â”€ reports/               # Generowanie raportÃ³w
â”‚       â”œâ”€â”€ analyzer.py        # Analiza wynikÃ³w
â”‚       â”œâ”€â”€ word_report.py     # Raport Word
â”‚       â””â”€â”€ report_generator.py
â”œâ”€â”€ raporty/                   # Folder na wygenerowane raporty
â”œâ”€â”€ main.py                    # GÅ‚Ã³wny plik uruchomieniowy
â”œâ”€â”€ requirements.txt           # ZaleÅ¼noÅ›ci Python
â””â”€â”€ README.md                  # Ten plik

```

## ğŸ”§ Instalacja

1. Sklonuj repozytorium:
```bash
git clone <repo-url>
cd Compass
```

2. Zainstaluj zaleÅ¼noÅ›ci:
```bash
pip install -r requirements.txt
```

3. (Opcjonalnie) Skonfiguruj zmienne Å›rodowiskowe:
```bash
export OPENAI_API_KEY="twÃ³j-klucz-api"
export PAGESPEED_API_KEY="twÃ³j-klucz-pagespeed"
```

## âš™ï¸ Konfiguracja

Edytuj plik `compass/config.py` aby dostosowaÄ‡ parametry:

```python
START_URL = "https://example.com/"  # URL do audytu
MAX_PAGES = 300                      # Maksymalna liczba stron
MAX_DEPTH = 3                        # Maksymalna gÅ‚Ä™bokoÅ›Ä‡ crawlingu
CONCURRENCY = 10                     # Liczba rÃ³wnolegÅ‚ych requestÃ³w

USE_PAGESPEED = False                # WÅ‚Ä…cz PageSpeed Insights
USE_AI_SUMMARY = True                # WÅ‚Ä…cz AI Summary

DOMAIN_SCOPE = "root"                # "root" lub "sub" (subdomeny)
```

## ğŸš€ UÅ¼ycie

### Podstawowe uÅ¼ycie

```bash
python main.py
```

### Jako moduÅ‚ Python

```python
import asyncio
from compass.crawler import crawl
from compass.reports import save_reports
from compass.config import START_URL, get_output_dir

# Uruchom crawling
data = asyncio.run(crawl(START_URL))

# Wygeneruj raporty
output_dir = get_output_dir()
save_reports(data, START_URL, output_dir)
```

## ğŸ“Š Generowane raporty

Wszystkie raporty sÄ… zapisywane w folderze `raporty/audyt_YYYY-MM-DD_HH-MM-SS/`:

1. **raport_dla_klienta.docx** - Profesjonalny raport Word dla klienta
2. **raport_szczegolowy.json** - PeÅ‚ne dane w formacie JSON
3. **raport_tabela.csv** - Dane tabelaryczne do analizy

## ğŸ” ModuÅ‚y

### Utils
- `url_utils.py` - Normalizacja URL, sprawdzanie domeny, wykluczenia
- `text_utils.py` - Czyszczenie tekstu HTML

### Analyzers
- `meta_analyzer.py` - Analiza title i description
- `nap_analyzer.py` - Local SEO (Name, Address, Phone)
- `eeat_analyzer.py` - SygnaÅ‚y E-E-A-T (Experience, Expertise, Authoritativeness, Trustworthiness)
- `security_analyzer.py` - NagÅ‚Ã³wki HTTP, SSL, bezpieczeÅ„stwo

### Crawler
- `fetcher.py` - Asynchroniczne pobieranie i parsowanie stron
- `robots.py` - ObsÅ‚uga robots.txt i sitemap.xml
- `crawler.py` - GÅ‚Ã³wny silnik crawlera z BFS

### Integrations
- `openai_integration.py` - Generowanie AI Summary przez OpenAI
- `pagespeed.py` - Google PageSpeed Insights API

### Reports
- `analyzer.py` - Analiza duplikatÃ³w i problemÃ³w
- `word_report.py` - Generator raportu Word (DOCX)
- `report_generator.py` - Orkiestracja wszystkich raportÃ³w

## ğŸ“ PrzykÅ‚adowa konfiguracja dla rÃ³Å¼nych scenariuszy

### Audyt maÅ‚ej strony
```python
MAX_PAGES = 50
MAX_DEPTH = 2
CONCURRENCY = 5
```

### Audyt duÅ¼ej strony
```python
MAX_PAGES = 1000
MAX_DEPTH = 5
CONCURRENCY = 20
```

### Audyt tylko gÅ‚Ã³wnych stron
```python
MAX_PAGES = 100
MAX_DEPTH = 1
EXCLUDED_PATTERNS = [
    r'/blog/',
    r'/archiwum/',
    # ... wiÄ™cej wzorcÃ³w
]
```

## ğŸ¤ WkÅ‚ad w projekt

Pull requesty sÄ… mile widziane! Przed wysÅ‚aniem PR:

1. Upewnij siÄ™, Å¼e kod jest zgodny z PEP 8
2. Dodaj testy dla nowych funkcji
3. Zaktualizuj dokumentacjÄ™

## ğŸ“„ Licencja

MIT License - zobacz plik LICENSE

## ğŸ™ PodziÄ™kowania

- BeautifulSoup4 - parsowanie HTML
- aiohttp - asynchroniczne HTTP
- extruct - ekstrakcja strukturalnych danych
- python-docx - generowanie raportÃ³w Word

## ğŸ“ Kontakt

W razie pytaÅ„ lub problemÃ³w, otwÃ³rz issue na GitHubie.

---

**Compass** - TwÃ³j przewodnik w Å›wiecie audytÃ³w SEO/AEO/GEO ğŸ§­
